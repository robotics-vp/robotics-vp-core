# Video-to-Policy Pipeline: Implementation Status

**Last Updated**: 2025-01-12

---

## âœ… COMPLETED: Step 1 & 2 - Video Infrastructure

### What's Working

**1. Video Encoder Module** (`src/encoders/video_encoder.py`)
- âœ… Simple2DCNN (651K params, CPU-friendly)
- âœ… Simple3DCNN (837K params)
- âœ… R3D18 (requires torchvision + GPU)
- âœ… Unified interface: `(B, T, C, H, W) â†’ (B, latent_dim)`

**2. Encoder Builder** (`src/encoders/builder.py`)
- âœ… Config-driven encoder construction
- âœ… Supports both MLP (state) and Video encoders
- âœ… Device handling (CPU/GPU)

**3. DishwashingVideoEnv Wrapper** (`src/envs/video_wrappers.py`)
- âœ… Wraps DishwashingEnv to emit video observations
- âœ… Synthetic frame generation from state (colored bars)
- âœ… Frame buffer management (temporal stacking)
- âœ… Returns: `(T, C, H, W)` video instead of state vector
- âœ… Tested and validated

**4. Video Configuration** (`configs/dishwashing_video.yaml`)
- âœ… encoder.type = "video"
- âœ… env.type = "dishwashing_video"
- âœ… All video parameters specified

### Testing Results

```bash
$ python3 src/envs/video_wrappers.py

Testing DishwashingVideoEnv...

[Reset Test]
Observation shape: (8, 3, 64, 64)  âœ…
Observation dtype: float32          âœ…
Observation range: [0.000, 0.941]   âœ…

[Step Test]
Observation shape: (8, 3, 64, 64)  âœ…
Info keys: ['succ', 'errs', 'p_err', 'speed', 'care', 'rate_per_min']  âœ…

âœ… All tests passed!
```

---

## ğŸ”„ IN PROGRESS: Step 3 - SAC Integration

### What Needs to Be Done

**Modify `train_sac.py` to support both state and video modes:**

1. **Environment Creation** (based on config)
```python
if cfg['env']['type'] == 'dishwashing':
    # State mode (current)
    env = DishwashingEnv(params)
    obs_dim = env._obs().shape[0]
    encoder = build_encoder(cfg['encoder'], obs_dim=obs_dim, device=device)

elif cfg['env']['type'] == 'dishwashing_video':
    # Video mode (new)
    from src.envs.video_wrappers import create_video_env
    env = create_video_env(
        base_env_class=DishwashingEnv,
        base_env_config={'params': params},
        video_config=cfg['env']['video']
    )
    encoder = build_encoder(cfg['encoder'], video_shape=(8,3,64,64), device=device)
```

2. **Observation Handling in Replay Buffer**
```python
# Current: buffer stores state vectors
# New: buffer should store video observations (8, 3, 64, 64)

# When sampling batch:
if encoder_type == 'video':
    obs_batch = torch.FloatTensor(obs_batch).to(device)  # (B, T, C, H, W)
else:
    obs_batch = torch.FloatTensor(obs_batch).to(device)  # (B, obs_dim)

# Encode to latent
z = encoder(obs_batch)  # (B, latent_dim) for both modes
```

3. **Novelty & Î”MPL on Latents**
```python
# OLD (state-based):
novelty = compute_novelty(state)

# NEW (latent-based):
z = encoder(obs)  # obs is either state or video
novelty = compute_novelty(z)  # operates on latent space

# Î”MPL estimation
delta_mpl_cust = data_value_estimator.predict(novelty)
```

4. **Episode Loop Updates**
```python
# Reset
obs = env.reset()  # (8,3,64,64) in video mode, (obs_dim,) in state mode

# Step
obs_next, info, done = env.step(action)  # Same signature for both modes

# Store in replay
buffer.add(obs, action, reward, obs_next, done)  # Works for both
```

### Key Design Principle

**SAC operates ONLY on latent embeddings `z`, never on raw observations.**

This means:
- Actor network: `Ï€(a | z)` not `Ï€(a | obs)`
- Critic network: `Q(z, a)` not `Q(obs, a)`
- Encoder handles modality: `z = f_Ïˆ(obs)` where obs can be state or video

---

## ğŸ“‹ TODO: Steps 4-6

### Step 4: Diffusion Novelty on Latents

**Modify**: `src/deep_learning/novelty_diffusion.py`

```python
class DiffusionNoveltyEstimator:
    """Operates on latent embeddings, not raw pixels"""

    def __init__(self, latent_dim=128):
        # Diffusion operates in latent space
        self.latent_dim = latent_dim
        self.prior = LatentDiffusion(latent_dim)

    def compute_novelty(self, z):
        """
        Args:
            z: (latent_dim,) embedding from encoder
        Returns:
            novelty: float
        """
        # Compute reconstruction error in latent space
        ...
```

**Integration** (train_sac.py):
```python
# Encode observation to latent
z = encoder(obs)

# Compute novelty in latent space
novelty = diffusion_novelty.compute_novelty(z)

# Predict Î”MPL from novelty
delta_mpl_cust = data_value_estimator.predict(novelty)

# Use in spread allocation
spread_info = compute_spread_allocation(
    w_hat_r=w_hat_r,
    w_h=wh,
    time_hours=time_hours,
    delta_mpl_cust=delta_mpl_cust,
    delta_mpl_total=delta_mpl_total
)
```

### Step 5: State vs Video Comparison

**Create**: `experiments/compare_state_vs_video.py`

```python
def compare_modalities():
    """
    Run short training with state vs video configs.
    Compare:
    - Final MP, error, wage parity
    - Consumer surplus distribution
    - Training stability (loss variance)
    """

    results = {}

    # Run state mode
    results['state'] = run_training('configs/dishwashing_feasible.yaml', episodes=200)

    # Run video mode
    results['video'] = run_training('configs/dishwashing_video.yaml', episodes=200)

    # Compare metrics
    compare_economics(results['state'], results['video'])
    compare_stability(results['state'], results['video'])

    # Generate report
    save_comparison_report(results, 'experiments/state_vs_video_comparison.json')
```

### Step 6: GPU + Physics Scaffolding

**Create**: `requirements-gpu.txt`
```
torch>=2.0.0+cu118
torchvision>=0.15.0+cu118
diffusers>=0.21.0
```

**Create**: `GPU_README.md` (instructions for RunPod/AWS)

**Create**: `src/envs/physics/__init__.py` (interface stub)

---

## Current Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VIDEO-TO-POLICY PIPELINE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Video Obs]                    [State Obs]
(8, 3, 64, 64)                 (obs_dim,)
     â”‚                              â”‚
     â”‚                              â”‚
     â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Video Encoderâ”‚            â”‚ MLP Encoder  â”‚
â”‚ Simple2DCNN  â”‚            â”‚  (256-256)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
         [Latent z] (128-dim)
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SAC   â”‚ â”‚Novelty â”‚ â”‚   Î”MPL     â”‚
â”‚ Policy â”‚ â”‚Estimateâ”‚ â”‚ Estimator  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
         [Economics Layer]
    (Wage parity, spread, CS)
```

**Key Insight**: Everything downstream of encoder is modality-agnostic.

---

## File Summary

### Created Files
| File | Status | Purpose |
|------|--------|---------|
| `src/encoders/video_encoder.py` | âœ… Complete | Video encoder implementations |
| `src/encoders/builder.py` | âœ… Complete | Unified encoder builder |
| `src/envs/video_wrappers.py` | âœ… Complete | Video environment wrapper |
| `configs/dishwashing_video.yaml` | âœ… Complete | Video mode configuration |
| `VIDEO_TO_POLICY_ROADMAP.md` | âœ… Complete | Implementation roadmap |
| `VIDEO_PIPELINE_STATUS.md` | âœ… Complete | This document |

### Modified Files
| File | Modifications Needed | Status |
|------|---------------------|--------|
| `train_sac.py` | Add video/state mode switching | ğŸ”„ TODO |
| `src/deep_learning/novelty_diffusion.py` | Operate on latents not pixels | ğŸ”„ TODO |

### New Files Needed
| File | Purpose | Status |
|------|---------|--------|
| `experiments/compare_state_vs_video.py` | Modality comparison | ğŸ“‹ TODO |
| `requirements-gpu.txt` | GPU dependencies | ğŸ“‹ TODO |
| `GPU_README.md` | Cloud training guide | ğŸ“‹ TODO |
| `src/envs/physics/__init__.py` | Physics env interface | ğŸ“‹ TODO |

---

## Next Actions

### Immediate (This Week)
1. âœ… **DONE**: Video encoder + wrapper + config
2. **TODO**: Modify `train_sac.py` for video mode support
3. **TODO**: Test short training run (50 episodes) in video mode
4. **TODO**: Verify economics unchanged (wage parity, CS, spread)

### Short-term (Next Week)
1. **TODO**: Wire diffusion novelty to latents
2. **TODO**: Run state vs video comparison (200 episodes each)
3. **TODO**: Create GPU requirements + README

### Medium-term (Next Month)
1. **TODO**: Set up RunPod/AWS GPU training
2. **TODO**: Test R3D18 encoder on GPU
3. **TODO**: Scaffold PyBullet physics env

---

## Key Validation Criteria

Before declaring video mode "complete":

1. âœ… Video encoder works (tested standalone)
2. âœ… Video wrapper works (tested standalone)
3. â³ SAC trains on video observations
4. â³ Economics metrics match state mode (Â±5% tolerance)
5. â³ Novelty/Î”MPL operates on latents
6. â³ State vs video comparison shows equivalence

---

## Why This Matters

**Before**: Training on hand-crafted state vectors (not scalable to real robots)

**After**: Training on video observations (enables real demonstrations, transfer learning)

**Impact**:
- Can use YouTube demonstrations
- Can use kinesthetic teaching
- Can transfer simâ†’real via vision
- **Core thesis validated**: "Economics-driven video-to-policy learning"

---

**Status**: Infrastructure complete, integration in progress.

**Blocking**: None - all dependencies ready.

**ETA**: Video mode fully working within 1-2 days of focused work.
